{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9850, 299, 299, 3)\n",
      "(9850, 6)\n",
      "(7880, 299, 299, 3) (7880, 6)\n",
      "(1970, 299, 299, 3) (1970, 6)\n"
     ]
    }
   ],
   "source": [
    "X_list = []\n",
    "for filename in sorted(os.listdir('./Dataset_Resize/')):\n",
    "    img = image.load_img(Path('./Dataset_Resize/', filename))\n",
    "    X_list.append(image.img_to_array(img))\n",
    "X = np.array(X_list)\n",
    "print(X.shape)\n",
    "\n",
    "df = pd.read_csv('./vehicles.csv', index_col=0)\n",
    "df_dummy = pd.get_dummies(df['Vehicle_Type'])\n",
    "Y = np.array(df_dummy)\n",
    "print(Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"global_average_pooling2d_1/Mean:0\", shape=(?, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(include_top = False, weights = 'imagenet')\n",
    "x = base_model.output\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024,activation='relu')(x)\n",
    "predictions = Dense(6,activation='softmax')(x)\n",
    "model = Model(inputs = base_model.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Setup_TL(base_model, model):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7880 samples, validate on 1970 samples\n",
      "Epoch 1/3\n",
      "7880/7880 [==============================] - 1780s 226ms/step - loss: 0.8756 - acc: 0.7089 - val_loss: 13.7934 - val_acc: 0.0817\n",
      "Epoch 2/3\n",
      "7880/7880 [==============================] - 1822s 231ms/step - loss: 0.6465 - acc: 0.7792 - val_loss: 10.9247 - val_acc: 0.2518\n",
      "Epoch 3/3\n",
      "7880/7880 [==============================] - 1783s 226ms/step - loss: 0.5483 - acc: 0.8165 - val_loss: 10.7891 - val_acc: 0.2690\n"
     ]
    }
   ],
   "source": [
    "model = Setup_TL(base_model, model)\n",
    "history_1 = model.fit( X_train, Y_train,                      \n",
    "                       batch_size=256,\n",
    "                       epochs=3,\n",
    "                       verbose=1,\n",
    "                       validation_data=(X_test, Y_test),\n",
    "                       shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. <br>\n",
    "We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from Inception model: the last activation maps before the fully-connected layers) in two numpy arrays. <br>\n",
    "Then we will train a small fully-connected model on top of the stored features<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Inception V3\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "model_Inception = InceptionV3(include_top = False, weights = 'imagenet')   \n",
    "print(\"load Inception V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate train features\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow(X_train, Y_train, \n",
    "                     batch_size=batch_size)\n",
    "\n",
    "bottleneck_features_train = model_Inception.predict_generator(generator)\n",
    "print(\"Calculate train features\")\n",
    "print(bottleneck_features_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train features\n"
     ]
    }
   ],
   "source": [
    "np.savez('inception_features_train', features=bottleneck_features_train)\n",
    "# np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "print(\"Saved train features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate test features\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow(X_test, Y_test, \n",
    "                     batch_size=batch_size)\n",
    "\n",
    "bottleneck_features_validation = model_Inception.predict_generator(generator)\n",
    "print(\"Calculate test features\")\n",
    "print(bottleneck_features_validation.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test features\n"
     ]
    }
   ],
   "source": [
    "np.savez('inception_features_validation', features=bottleneck_features_validation)\n",
    "# np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n",
    "print(\"Saved test features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_Stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    verbose=0, \n",
    "    mode='auto'\n",
    ")\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "checkpointer = ModelCheckpoint(filepath=\"TL_Inception.hdf5\", verbose=1, save_best_only = True)\n",
    "csv = CSVLogger('TL_Inception.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " load our saved data and train a small fully-connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "(7880, 8, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "features_train = np.load('inception_features_train.npz')['features']\n",
    "print(\"done\")\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "(1970, 8, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "features_validation = np.load('inception_features_validation.npz')['features']\n",
    "print(\"done\")\n",
    "print(features_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7880, 6) (1970, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_add_top_layer(input_shape):  \n",
    "    \n",
    "    model_addtop = Sequential()\n",
    "    model_addtop.add(GlobalAveragePooling2D(input_shape = input_shape))\n",
    "    model_addtop.add(Dense(1024,activation='relu'))\n",
    "    model_addtop.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model_addtop.summary()\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "    \n",
    "    model_addtop.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model_addtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 2,104,326\n",
      "Trainable params: 2,104,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7880 samples, validate on 1970 samples\n",
      "Epoch 1/100\n",
      "7880/7880 [==============================] - 8s 1ms/step - loss: 1.5802 - acc: 0.5539 - val_loss: 1.2923 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.29791 to 1.29229, saving model to TL_Inception.hdf5\n",
      "Epoch 2/100\n",
      "7880/7880 [==============================] - 5s 693us/step - loss: 1.3253 - acc: 0.5829 - val_loss: 1.2999 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/100\n",
      "7880/7880 [==============================] - 5s 696us/step - loss: 1.3194 - acc: 0.5829 - val_loss: 1.2947 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "7880/7880 [==============================] - 6s 699us/step - loss: 1.3214 - acc: 0.5829 - val_loss: 1.2958 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "7880/7880 [==============================] - 6s 723us/step - loss: 1.3145 - acc: 0.5829 - val_loss: 1.3296 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "7880/7880 [==============================] - 6s 703us/step - loss: 1.3050 - acc: 0.5829 - val_loss: 1.3007 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "model_TL = model_add_top_layer(features_train.shape[1:])\n",
    "history_InceptionV3TL = model_TL.fit(features_train, Y_train,\n",
    "               batch_size=256,\n",
    "               epochs=100,\n",
    "               verbose=1,\n",
    "               validation_data=(features_validation, Y_test),\n",
    "               shuffle=True,\n",
    "               callbacks=[early_Stopping,reduceLR,checkpointer,csv])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
